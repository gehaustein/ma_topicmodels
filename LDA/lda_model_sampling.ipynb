{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=\"model.log\", format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.warning('Watch out!')  # will print a message to the console\n",
    "logging.info('I told you so')  # will not print anything\n",
    "\n",
    "import nltk; nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/complete_data_by_speech.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = df.sort_values([\"date\", \"intervention_id\"])\n",
    "# filter_df = filter_df[[\"date\",\"intervention_id\",\"text\",\"mep_id\",\"full_name\",\"role\",\"is_mep\",\"langdetect\",\"langid\"]]\n",
    "filter_df[\"lang_checkup\"] = np.where(filter_df[\"langdetect\"] == filter_df[\"langid\"], True, False)\n",
    "filter_df = filter_df[filter_df[\"langdetect\"]==\"en\"]\n",
    "filter_df = filter_df[filter_df[\"is_mep\"]==True]\n",
    "filter_df = filter_df.reset_index(drop=True)\n",
    "\n",
    "print(filter_df.shape)\n",
    "filter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = filter_df.text.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "# data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "# data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "# Remove distracting double quotes\n",
    "data = [re.sub(\"\\\"\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in tqdm(sentences):\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(len(data_words))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    print(\"stopwords\")\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in tqdm(texts)]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    print(\"bigrams\")\n",
    "    return [bigram_mod[doc] for doc in tqdm(texts)]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    print(\"trigrams\")\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in tqdm(texts)]\n",
    "\n",
    "def lemmatization(texts, allowed_postags):\n",
    "    print(\"lemmatization\")\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in tqdm(texts):\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Form Trigrams\n",
    "data_words_trigrams = make_trigrams(data_words_bigrams)\n",
    "\n",
    "# Initialize spacy 'en_core_web_sm' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Do lemmatization keeping only nouns\n",
    "data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN'])\n",
    "\n",
    "print(data_lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data_lemmatized\n",
    "with open(\"data_lemmatized\", \"w\") as fp:\n",
    "    json.dump(data_lemmatized, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized_list = []\n",
    "\n",
    "for i in tqdm(data_lemmatized):\n",
    "    lemma_str = ' '.join(i)\n",
    "    data_lemmatized_list.append(lemma_str)\n",
    "\n",
    "data_lemmatized_df = pd.DataFrame(data_lemmatized_list, columns=[\"text\"])\n",
    "\n",
    "data_lemmatized_df = data_lemmatized_df.join(filter_df.drop([\"text\"], axis = 1))\n",
    "data_lemmatized_df.head()\n",
    "\n",
    "data_lemmatized_df.to_csv(\"../dataset/data_lemmatized_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save corpus\n",
    "with open(\"corpus\", \"w\") as fp:\n",
    "    json.dump(corpus, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load resources if necessary\n",
    "\n",
    "# with open(\"corpus\", \"r\") as fp:\n",
    "#     corpus = json.load(fp)\n",
    "\n",
    "# with open(\"data_lemmatized\", \"r\") as fp:\n",
    "#     data_lemmatized = json.load(fp)\n",
    "\n",
    "id2word = corpora.Dictionary(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_no = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed_no)\n",
    "random_training_index = random.sample(range(0,len(corpus),1), int(len(corpus)/10))\n",
    "random_training_index.sort()\n",
    "print(random_training_index[:10], len(random_training_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_training_index\", \"w\") as fp:\n",
    "    json.dump(random_training_index, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_training_index_df = pd.DataFrame(random_training_index, columns=[\"training_index\"])\n",
    "random_training_index_df.to_csv(\"../dataset/random_training_index_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = [corpus[index] for index in random_training_index]\n",
    "len(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list = []\n",
    "\n",
    "for i in range(len(training_corpus))\n",
    "    if len(training_corpus[i])==0: #check for empty document\n",
    "        remove_list.append(i)\n",
    "\n",
    "len(remove_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in sorted(remove_list, reverse=True):\n",
    "    del training_corpus[index]\n",
    "\n",
    "len(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_lda_model(n_topics):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=training_corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=n_topics, \n",
    "                                           random_state=seed_no,\n",
    "                                           update_every=1,\n",
    "                                           passes=50)\n",
    "    \n",
    "    lda_model.save(f\"models_{seed_no}/lda_model_{n_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_topics = 2\n",
    "max_topics = 40\n",
    "steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_topics in tqdm(range(min_topics,max_topics+1,steps)):\n",
    "    train_and_save_lda_model(n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_list = []\n",
    "\n",
    "def calculate_coherence(n_topics):\n",
    "    loaded_model = gensim.models.ldamodel.LdaModel.load(f\"models_{seed_no}/lda_model_{n_topics}\")\n",
    "    cm = gensim.models.coherencemodel.CoherenceModel(model=loaded_model, corpus=corpus, coherence=\"c_v\", texts=data_lemmatized)\n",
    "    coherence = cm.get_coherence()\n",
    "    coherence_list.append([n_topics, coherence])\n",
    "\n",
    "for n_topics in tqdm(range(min_topics, max_topics+1, steps)):\n",
    "    calculate_coherence(n_topics)\n",
    "\n",
    "coherence_df = pd.DataFrame(coherence_list, columns=[\"topics\", \"coherence\"])\n",
    "\n",
    "coherence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_df.to_csv(\"visualisation/coherence_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_df.plot.line(x='topics', y='coherence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42: 15, 16, 19, **34 (Topic 26)**\n",
    "404: 35, 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore different models\n",
    "n_topics = 34\n",
    "\n",
    "with open(f\"models_{seed_no}/lda_model_{n_topics}.id2word\", \"rb\") as fp:\n",
    "    id2word = pickle.load(fp)\n",
    "\n",
    "model_load = gensim.models.ldamodel.LdaModel.load(f\"models_{seed_no}/lda_model_{n_topics}\")\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(model_load.print_topics(num_topics=n_topics))\n",
    "doc_lda = model_load[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_no = 26\n",
    "topic_df = pd.DataFrame(model_load.show_topic(topic_no, topn=100), columns = [\"word\", \"probability\"])\n",
    "topic_df\n",
    "topic_df.to_csv(f\"visualisation/model_{n_topics}_topic_{topic_no}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_list_complete=[]\n",
    "\n",
    "for doc in tqdm(corpus):\n",
    "    prob_dict = model_load.get_document_topics(doc)\n",
    "    prob_list=[]\n",
    "    for i in range(0,n_topics,1):\n",
    "        try:\n",
    "            prob_list.append(dict(prob_dict)[i])\n",
    "        except KeyError:\n",
    "            prob_list.append(None)\n",
    "    prob_list_complete.append(prob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = pd.DataFrame(prob_list_complete)\n",
    "\n",
    "colnames = []\n",
    "for i in range(0,n_topics,1):\n",
    "    colnames.append(f\"topic_{i}\")\n",
    "prob_df.columns=colnames\n",
    "\n",
    "prob_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior_df = filter_df[filter_df.index.isin(random_training_index)].reset_index(drop=True)\n",
    "# posterior_df = filter_df[~filter_df.index.isin(remove_list)].reset_index(drop=True)\n",
    "posterior_df = filter_df.join(prob_df)\n",
    "posterior_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refugee_df = posterior_df[posterior_df[\"topic_26\"]>=0.30]\n",
    "refugee_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "vis = pyLDAvis.gensim_models.prepare(model_load, corpus, id2word)\n",
    "pyLDAvis.save_html(vis, f\"vis_{n_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d99cafe4c174b2b44bd35f69a9f9f4db6ae957380b95445e310397cf4491890f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ma_condaenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
